{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>AI-Exam</h1>\n",
    "<p>For this exam project we decided to work with natural language processing, i.e text processing. We wanted to determaine if a strong correlation could be made between the overview/description of a movie and its genre. The overall idea was to make the genre of the movies our <i>dependable variable</i>, hence transforming all genres into numeric values, so that each number represents a different genre. Then, we would break the down the description of each movie, initially removing all stop words, i.e words that are concidered to have no siginificant or descriptive meaning of a text. Then check how frequently the remaining words appeared in each movie description, and assign them a \"weight\" accordingly. This way we would be able to predict the genre of a movie, based off of the movie description itself.</p>\n",
    "\n",
    "<h2>The dataset</h2>\n",
    "We've downloaded a dataset from kaggle, which can be found [here](https://www.kaggle.com/tmdb/tmdb-movie-metadata). The dataset contains movies from [IMDb](https://www.imdb.com/), and meta data about them. Most importantly it contains movies with their corresponding genres and a brief description about them. Initially we used another dataset where each movie was associated with one genre, but we got very poor results, and opted to change dataset, because we believed that the poor result primarily was a side effect of the size of the dataset. This was somewhat confirmed by the change of dataset, seeing as our models improved across the board after changing to a larger dataset. However the change did not come without issues. As briefly mentioned, in the initial dataset all movies were only related to <b>one</b> genre, whereas in the new dataset a movie could be related to <b>several</b> genres. This is an issue, as we only can have one dependable variable. To get around this issue, we decided that the first genre in the genre array, would be the movies genre, well knowning that the result potentially would be scewerd abit, as there's no guarentee the first genre in the array is the genre that fits the movie best. Furthermore it also means that our algorithm potentially guesses correct with the second or third, or even fourth genre, but it would still be classified as a false negative in our confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mwe1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "\n",
    "import pandas as pd\n",
    "import re #Regular expresion\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import nltk #Natural language processing tool kit\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Modelling\n",
    "from sklearn import model_selection, preprocessing, naive_bayes, metrics, svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "# Data preprocessing\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Validation\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Helper methods</h2>\n",
    "<p>Three different helper methods has been developed during this project, to get the best possible results</p>\n",
    "\n",
    "<h3>train_model</h3>\n",
    "<p>The 'train_model' method was developed to get an easier overview, seeing as we're training a bunch of differnt models, which are all trained in the same way. Hence the method takes the following parameters: classfier, X_train, y_train and X_test. This allows to train several models in bulk, whilst maintaining an overview.</p>\n",
    "\n",
    "<h3>convert_column_array_to_normal_array</h3>\n",
    "<p>As the method name suggests, the method takes an array, which is shaped like so (x, 1), and converts to an array with the following shape (1, y). This is needed for some of the training models.</p>\n",
    "\n",
    "<h3>transform_genre</h3>\n",
    "<p>TBD</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper methods\n",
    "def train_model(classifier, X_train, y_train, X_test):\n",
    "    \n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    pred = classifier.predict(X_test)\n",
    "\n",
    "    return metrics.accuracy_score(y_test, pred)\n",
    "\n",
    "def convert_column_array_to_normal_array(arr):\n",
    "    vert_arr = []\n",
    "    for i in arr:\n",
    "        vert_arr.append(i)\n",
    "    return vert_arr\n",
    "\n",
    "def transform_genre(arr):\n",
    "    arr = [[],[]]\n",
    "    failedIndexes = []\n",
    "    counter = 0\n",
    "    while counter < len(arr):\n",
    "        try:\n",
    "            genreObject = array[counter]\n",
    "            genreStr = genreObject[0]            \n",
    "            genreStr = genreStr.replace('[', '')\n",
    "            genreStr = genreStr.replace(']','')\n",
    "            splitStr = genreStr.split('}, {')\n",
    "            arr.append(counter)\n",
    "            splitStr[0] = splitStr[0].replace('{', '')\n",
    "            length = len(splitStr)\n",
    "            splitStr[length-1] = splitStr[length-1].replace('}', '')\n",
    "            for i in splitStr:\n",
    "                i = \"{\"+i+\"}\"\n",
    "                jsonobj = json.loads(i)\n",
    "                genre = jsonobj[\"name\"]\n",
    "                arr[counter].append(genre)                      \n",
    "            counter += 1 \n",
    "        except:\n",
    "            failedIndexes.sort()\n",
    "            failedIndexes = failedIndexes[::-1] # reversing the list\n",
    "            counter += 1\n",
    "    return arr, failedIndexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data pre-processing</h2>\n",
    "<p>Kaggle gave us the option to select which columns we would like to download, hence our dataset came with only two columns out of the box; genre and overview. Which means we didn't have to make this split programatically. As briefly touched upon in the introduction, a movie could have several genres, which was represented by a genre array. As an example a movie could be both a Sci-fi and an Action movie, which also seems rather logical. As we've also mentioned the order of the genres doesn't nessecarily mean anything in terms of genre specificity, and this is important, as it makes our results error-prone. We convert the ovewview and the genres into list which were previously stored in a dataframe.\n",
    "\n",
    "Initally we create two empty lists, one list to hold all the genres that aren't empty, as well as a list to keep track of the indexes of genres that contain to data. We then start a while loop that iterates trough all the elements in our list of genres (y_list). We've defined a try/except clause, where, if possible, we store something we've called an \"genreObject\" which is actually a list of list only containing one element. Hence we grab that one element in the list, which we've named \"genreStr\", this element looks like a json object, but is in reality a poorly formatted string. We then contiue to split our poorly formatted string on white spaces, and grab the element at index position three, which is always the genre, given it's present. We then use regex to remove unwanted characters, which are included in our string, because as we mentioned, it looks like a json element, i.e we have curly braces and brackets in our stirng. As a biproduct of using regexs findall(), which return a list, we select the procssed string by selecting the element at index 0 the returned list, which always will be our genre. We then add that genre to a new list which we've called \"y_\", and finally we increment our counter, and repeat this process untill we have all of our genres. Given the genre array is empty we hit the except clause, where we store the index of the missing genre, and increment our and continue the while loop.\n",
    "\n",
    "We now grab the list of failed indexes and sort, using pythons inbuilt sort() method, which is possible because it's numeric data. We then reverse our array which is super in easy in python using slicing. This is nessecary, otherwise when we start removing rows in our lists, the loop would crash itself, if we removed indexes from low to high. We now itterate trough our list of overviews and remove all of the overviews with the same index position as our missing genres, this is nessecary because the two list has to be same length.\n",
    "\n",
    "Now we create an empty list called X_ and repeat the process mentioned above, now only with overviews instead. Afer that we stored X_ and y_ in X_list and y_list respectively, because we thought we were ready to fit our model with the data now. However we still had nan values in our X_list, so we converted X_lists to a dataframe, after its content had been formatted, and used isNull() which returns a bool, and stored those indexes that returned true, and removed them accordingly from both X_list and y_list.\n",
    "\n",
    "Now were ready to fit and transom our model, so we split our datasets in test and traning batches and used fit_transform() on our labelenconder.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_list length: 4772, y_list length: 4772\n",
      "3579, 1193, 3579, 1193\n"
     ]
    }
   ],
   "source": [
    "moviesDF = pd.read_csv('tmdb_movies.csv') \n",
    "\n",
    "# RETURN TO THIS LATER\n",
    "# moviesDF.groupby('genres').size() # prints how many of each genre there exists\n",
    "\n",
    "\n",
    "y = moviesDF[['genres']] # our dependent variable\n",
    "\n",
    "X = moviesDF[['overview']] # our independant variable\n",
    "\n",
    "X_list = X.values.tolist()\n",
    "y_list = y.values.tolist()\n",
    "\n",
    "#result, failed = TransformGenre(y_list)\n",
    "\n",
    "y_ = [] # temp, array for genres that arent empty\n",
    "failedIndexes = [] # array to keep track of the index we had an empty genre, to be used later in deletion.\n",
    "\n",
    "counter = 0\n",
    "# we iterate over y_list to find all genres that arent empty, if one is empty its gonna trigger an exception\n",
    "# which triggers our except clause. The except clause saved the index the error empty genre was located and progresses the counter\n",
    "while counter < len(y_list):\n",
    "    try:\n",
    "        genreObject = y_list[counter]\n",
    "        genreStr = genreObject[0]\n",
    "        strstr = genreStr.split()\n",
    "        indexedstr = strstr[3]\n",
    "        finalStr = re.findall(r'\\w+', indexedstr)    \n",
    "        y_.append(finalStr[0])\n",
    "        counter += 1 \n",
    "    except:\n",
    "        failedIndexes.append(counter)\n",
    "        counter += 1\n",
    "        \n",
    "failedIndexes.sort()\n",
    "failedIndexes = failedIndexes[::-1] # reversing the list\n",
    "# as it turns out when you delete an index from a python list, it collapses the list, so we had to delete the highest index first to circumvent this.\n",
    "for index in failedIndexes:\n",
    "    del X_list[index]\n",
    "    \n",
    "y_list = y_ \n",
    "\n",
    "X_ =  [] # temp list to contain all strings\n",
    "failedIndexes = []\n",
    "counter = 0\n",
    "# Currently X_list contains a collection of collections, this kind of list-ception is incompatible with the AI algorithm\n",
    "# So we create this small loop to extract the string.\n",
    "while counter < len(X_list):\n",
    "    listLine = X_list[counter]\n",
    "    listString = listLine[0]\n",
    "    if not listString:\n",
    "        failedIndexes.append(counter)\n",
    "        counter += 1\n",
    "        continue\n",
    "    X_.append(listString)\n",
    "    counter += 1\n",
    "    \n",
    "failedIndexes = failedIndexes[::-1]\n",
    "for i in failedIndexes:\n",
    "    del X_[i]\n",
    "    \n",
    "X_list = X_\n",
    "\n",
    "# We had nan values in our X_list, we decided to convert the X_list back to a dataframe\n",
    "# in order to run isnull(), which returns a list of false/true wether an entry is nan or not\n",
    "# with this we simply saved the index of which the nan occured and deleted\n",
    "# it from both our dependant and independant variable.\n",
    "\n",
    "tempDf = pd.DataFrame(X_list)\n",
    "boolList = tempDf.isnull().values\n",
    "\n",
    "badIndexes = []\n",
    "counter = 0\n",
    "while counter < len(boolList):\n",
    "    if boolList[counter][0]:\n",
    "        badIndexes.append(counter)               \n",
    "    counter += 1\n",
    "\n",
    "badIndexes = badIndexes[::-1]\n",
    "\n",
    "for i in badIndexes:\n",
    "    del X_list[i]\n",
    "    del y_list[i]\n",
    "          \n",
    "print(f'X_list length: {len(X_list)}, y_list length: {len(y_list)}')  \n",
    "        \n",
    "# we are splitting our dataset up here for training and later validation.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_list,y_list)\n",
    "\n",
    "print(f'{len(X_train)}, {len(X_test)}, {len(y_train)}, {len(y_test)}')\n",
    "\n",
    "# Encoder to encode our dependant variable\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Feature Engineering</h2>\n",
    "<p>Count Vector is a matrix notation of the dataset in which every row represents a document from the corpus, where corpus roughly translates to the training data. Every row represents a term from the corpus and has a corresponding value representing the frequency count of a particular term in a particular document. The count vectors have several different paramters which can be adjusted to yield different results, sometimes it's just about adjusting a paramter slightly, and suddenly results are amazing, but the same can be true in the oposit direction.</p>\n",
    "\n",
    "<h3>analyzer</h3>\n",
    "<p>analyzer is a paramter we set to equal to 'word'. Which means we only words, rather than if set it to 'char' we would concider every single char.</p>\n",
    "\n",
    "<h3>stop_words</h3>\n",
    "<p>We tried using the built in stopwords from the scikit toolkit, which can be accessed by setting the stop_words paramater equal to 'english'. Taking a closer look at the stopwords, we felt they were insufficient, and briefly we concidered making our own stopwords, untill we realized how much of a hassle that would be. Nltk, Natual Language Toolkit, also offers a list of stopwords, which we tried using, yieldig slightly better results, sometimes...<p/>\n",
    "\n",
    "<h3>token_pattern</h3>\n",
    "<p>Further more we tried adjusting the token pattern , which basically is a regular expression, expressing when a word is valid/to be concidered i.e having a token patter like: '\\w{1,}', means that all words with more than one character is concidered. Where as if we change the pattern to '\\w{3,}', we would only concider words with four or more characters. Finally we also tried to restricting the characters themselves, i.e adding the following to our token: '\\w{x,}[a-z]' which essetianlly means we only concider character between and a-z, i.e not numbers or special characters. We played around with different combinations, but found that not providing a token at all yilded the most consistent results.<p/>\n",
    "    \n",
    "<h3>ngram_range</h3>    \n",
    "<p>ngram_range defines how many words in a sentence are to be concidered as a 'unit'. Concider the following sentence: \"The mighty lion sleeps tonight\". If ngram_range was set to (1,2), our output of the sentence would be: \"The\":x, \"mighty\":x, ... \"The mighty\":x, \"mighty lion\":x, and so on. Adjusting this paramter clearly affected our results, and we would the most consistent result using ngram_range=(1,3). We opted our of using this paramter, because using analyzer='word' yileded better results.</p>\n",
    "    \n",
    "<h3>max_features</h3>\n",
    "<p>Adjusting this setting change the number of total features we would concider, again a paramter that clearly affected our results, and essetially we stopped playing with it, and settled at 5000, where we seemed to get consistent results.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop_words = set(stopwords.words('english'))\n",
    "#token = 'r\\w{4,}[a-z]'\n",
    "\n",
    "count_vect = CountVectorizer(analyzer='word', stop_words='english', max_features=5000)\n",
    "count_vect.fit(X_train)\n",
    "# Now we are gonna transform the training and test data with our vectorized object\n",
    "\n",
    "X_train_count = count_vect.transform(X_train)\n",
    "X_test_count = count_vect.transform(X_test)\n",
    "\n",
    "# Now we are gonna use Term Frequency - Inverted Document Frequence (TF-IDF) vectors as features\n",
    "# The score generate by the TF-IDF represents the relatuve importance of a term in a document and the entire corpus.\n",
    "# We generate this score in two steps:\n",
    "# The first computes the normalizeds term frequency (Tf) --- TF(x) = Number of times x appears in the document / total bynber if terms in the document. \n",
    "# the second computes the inverse document frequency  (IDF) --- IDF(x) = log_e(total number of documents / number of documents with term x in it)\n",
    "# as mentioned earlier we could have chosen to use an n-gram composed of words, which we have implemented in line 61.\n",
    "# now we are creating the TF-IDF score based on that n-gram.\n",
    "\n",
    "#tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern='\\w{1,}', max_features=5000)\n",
    "tfidf_vect = TfidfVectorizer(encoding='utf-8',lowercase=True, stop_words='english', sublinear_tf=True, use_idf=True,max_features=5000)\n",
    "tfidf_vect.fit(X_train)\n",
    "X_train_tfidf = tfidf_vect.transform(X_train)\n",
    "X_test_tfidf = tfidf_vect.transform(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training and testing</h2>\n",
    "<p>To train and test our models we used the helper method that we developed earlier on. To compare the results of the diferent models we printed out the accuracy of each model after it had run.</p>\n",
    "\n",
    "<h3>Naive Bayers: Multinominal</h3>\n",
    "<p>Naive bayers is a probolistic machine learning algorithm, that is particuallarly good at multiclassification. The algorithm is primarily based on bayers theorm which can be expressed as: P(A|B) = P(B|A)P(A)/P(B).\n",
    "Naive bayers is often used solve several different AI tasks, but is often seen in text reconigition, for example; when it comes determaining if something is spam or not. The 'Naive' part comes from the assumption that the features are independant of each other which means that the pressence of one feature does not affect the pressence of another.</p>\n",
    "    \n",
    "<h3>Support Vector Machine</h3>\n",
    "<p>A SVM (Support Vector Machine) looks at the outliers in a dataset, and tries to define a linear line, called the hyper plane, between two outliers to determine wether something can be classified as A or B. This is however often dificult to achieve on all datasets, hence the dimensional space is increased, untill a seperation can be made. There are several functions to achieve this effect, but overall it makes SVM rather costly to compute.<p>\n",
    "    \n",
    "<h3>Random Forest Classifier</h3>\n",
    "<p>Random Forest Classifier can be used for both classification and regression tasks, but is generally concidered better for classifications tasks. The algorithm takes advantage of descission tree, and performs better the more data is available, but is also rather cost heavy because there's no pruning of the trees. Consider the following example; training_data = [1,2,3,4,5,6], then one of the trees (t) might be given the following data: t1 = [2,3,2,2,4,4], this ensures tree diversity, whilst still maintaining the data relevance. Furthermore random forest implements feature randomness, which is easier describe, by describing how a tree 'normally' decides its path. Before a tree decides its path, it has to concider all previous features. Random forest, takes a path based on a random subset of features. The algorithm, when used for classification, has each tree voting for an outcome, and the aggregated outcome with the highest vote will be the result, where as when used for regression, the average vote will be the result.<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, Count vectors: 0.40067057837384745\n",
      "NB, tf-idf vectors: 0.3864207879295893\n",
      "SVM, Count vectors: 0.38222967309304273\n",
      "SVM, tf-idf vectors: 0.3889354568315172\n",
      "RF, Count vectors: 0.366303436714166\n",
      "RF, tf-idf vectors: 0.3688181056160939\n"
     ]
    }
   ],
   "source": [
    "label = convert_column_array_to_normal_array(y_train)\n",
    "\n",
    "accuracy = train_model(classifier = naive_bayes.MultinomialNB(),\n",
    "                       X_train = X_train_count,\n",
    "                       y_train = label,\n",
    "                       X_test = X_test_count)\n",
    "print(f'NB, Count vectors: {accuracy}')\n",
    "\n",
    "accuracy = train_model(classifier = naive_bayes.MultinomialNB(),\n",
    "                       X_train = X_train_tfidf,\n",
    "                       y_train = label,\n",
    "                       X_test = X_test_tfidf)\n",
    "print(f'NB, tf-idf vectors: {accuracy}')\n",
    "\n",
    "\n",
    "accuracy = train_model(classifier = svm.SVC(),\n",
    "                       X_train = X_train_count,\n",
    "                       y_train = label,\n",
    "                       X_test = X_test_count)\n",
    "print(f'SVM, Count vectors: {accuracy}')\n",
    "\n",
    "accuracy = train_model(classifier = svm.SVC(),\n",
    "                       X_train = X_train_tfidf,\n",
    "                       y_train = label,\n",
    "                       X_test = X_test_tfidf)\n",
    "print(f'SVM, tf-idf vectors: {accuracy}')\n",
    "\n",
    "accuracy = train_model(classifier = ensemble.RandomForestClassifier(),\n",
    "                       X_train = X_train_count,\n",
    "                       y_train = label,\n",
    "                       X_test = X_test_count)\n",
    "print(f'RF, Count vectors: {accuracy}')\n",
    "\n",
    "accuracy = train_model(classifier = ensemble.RandomForestClassifier(),\n",
    "                       X_train = X_train_tfidf,\n",
    "                       y_train = label,\n",
    "                       X_test = X_test_tfidf)\n",
    "print(f'RF, tf-idf vectors: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Conclusion</h2>\n",
    "<p>Several issues were raised during this project. Initially our dataset wasn't large enough to yield accurate results, hence we swapped to a larger dataset mid-project. The new dataset, as briefly touched upon in the introduction, contained several genres per movie, which raised new challenges. As we yet, don't know how to handle this problem, we opted to select the first out of n genres related to a movie, saying that, that one genre was the movies genre. This means our models might predict a genre, that in fact is related to the movie, according to the dataset, but is still clasified as a wrong prediction. As a result all of our models are rather inaccurate despite having, what we believe is, sufficient data preprocessing and model training.\n",
    "\n",
    "Naive Bayers was the first algorithm we implemented, and because of the low accuracy score, we opted to implemented SVM, which yielded an equally low score. Finally we implemented Random Forest, which yielded the lowest and most varying result of them all. Playing with the different parameters, as describe in the Feature Engineering section, we were unsuccessful in improving the results more than the current implementation.\n",
    "\n",
    "As a conclusion we believe that we've processed our data sufficiently, as well as trained our models correctly, but simply put we don't know how to handle muliple dependable variables.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Group</h2>\n",
    "<b>- Mikkel Wexøe Ertbjerg // cph-me209@cphbusiness.dk</b>\n",
    "\n",
    "<b>- Nikolai Sjhøholm Christiansen // cph-nc103@cphbusiness.dk</b>\n",
    "\n",
    "<b>- Nikolaj Dyring Jensen // cph-nj183@cphbusiness.dk</b>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
