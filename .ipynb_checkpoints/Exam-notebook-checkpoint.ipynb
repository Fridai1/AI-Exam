{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>AI-Exam</h1>\n",
    "<p>For this exam project we decided to work with natural language processing, i.e text processing. We wanted to determaine if a strong correlation could be made between the overview/description of a movie and its genre. The overall idea was to make the genre of the movies our <i>dependable variable</i>, hence transforming all genres into numeric values, so that each number represents a different genre. Then, we would break the down the description of each movie, initially removing all stop words, i.e words that are concidered to have no siginificant or descriptive meaning of a text. Then check how frequently the remaining words appeared in each movie description, and assign them a \"weight\" accordingly. This way we would be able to predict the genre of a movie, based off of the movie description itself.</p>\n",
    "\n",
    "<h2>The dataset</h2>\n",
    "We've downloaded a dataset from kaggle, which can be found [here](https://www.kaggle.com/tmdb/tmdb-movie-metadata). The dataset contains movies from [IMDb](https://www.imdb.com/), and meta data about them. Most importantly it contains movies with their corresponding genres and a brief description about them. Initially we used another dataset where each movie was associated with one genre, but we got very poor results, and opted to change dataset, because we believed that the poor result primarily was a side effect of the size of the dataset. This was somewhat confirmed by the change of dataset, seeing as our models improved across the board after changing to a larger dataset. However the change did not come without issues. As briefly mentioned, in the initial dataset all movies were only related to <b>one</b> genre, whereas in the new dataset a movie could be related to <b>several</b> genres. This is an issue, as we only can have one dependable variable. To get around this issue, we decided that the first genre in the genre array, would be the movies genre, well knowning that the result potentially would be scewerd abit, as there's no guarentee the first genre in the array is the genre that fits the movie best. Furthermore it also means that our algorithm potentially guesses correct with the second or third, or even fourth genre, but it would still be classified as a false negative in our confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "import pandas as pd\n",
    "import re #Regular expresion\n",
    "import json\n",
    "import numpy as np\n",
    "import nltk #Natural language processing tool kit\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Modelling\n",
    "from sklearn import model_selection, preprocessing, naive_bayes, metrics, svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "# Data preprocessing\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Validation\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Helper methods</h2>\n",
    "<p>Three different helper methods has been developed during this project, to get the best possible results</p>\n",
    "\n",
    "<h3>train_model</h3>\n",
    "<p>The 'train_model' method was developed to get an easier overview, seeing as we're training a bunch of differnt models, which are all trained in the same way. Hence the method takes the following parameters: classfier, X_train, y_test and X_test. This allows to train several models in bulk, whilst maintaining an overview.</p>\n",
    "\n",
    "<h3>convert_column_array_to_normal_array</h3>\n",
    "<p>As the method name suggests, the method takes an array, which is shaped like so (x, 1), and converts to an array with the following shape (1, y). This is needed for some of the training models.</p>\n",
    "\n",
    "<h3>transform_genre</h3>\n",
    "<p>TBD</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper methods\n",
    "def train_model(classifier, X_train, y_test, X_test):\n",
    "    \n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(X_train, y_test)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    pred = classifier.predict(X_test)\n",
    "\n",
    "    return metrics.accuracy_score(y_test, pred)\n",
    "\n",
    "def convert_column_array_to_normal_array(arr):\n",
    "    arr = []\n",
    "    for i in array:\n",
    "        arr.append(i)\n",
    "    return arr\n",
    "\n",
    "def transform_genre(array):\n",
    "    arr = [[],[]]\n",
    "    failedIndexes = []\n",
    "    counter = 0\n",
    "    while counter < len(array):\n",
    "        try:\n",
    "            genreObject = array[counter]\n",
    "            genreStr = genreObject[0]            \n",
    "            genreStr = genreStr.replace('[', '')\n",
    "            genreStr = genreStr.replace(']','')\n",
    "            splitStr = genreStr.split('}, {')\n",
    "            arr.append(counter)\n",
    "            splitStr[0] = splitStr[0].replace('{', '')\n",
    "            length = len(splitStr)\n",
    "            splitStr[length-1] = splitStr[length-1].replace('}', '')\n",
    "            for i in splitStr:\n",
    "                i = \"{\"+i+\"}\"\n",
    "                jsonobj = json.loads(i)\n",
    "                genre = jsonobj[\"name\"]\n",
    "                arr[counter].append(genre)                      \n",
    "            counter += 1 \n",
    "        except:\n",
    "            failedIndexes.sort()\n",
    "            failedIndexes = failedIndexes[::-1] # reversing the list\n",
    "            counter += 1\n",
    "    return arr, failedIndexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data pre-processing</h2>\n",
    "<p>Brief explanations of choises and challenges</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "moviesDF = pd.read_csv('tmdb_movies.csv') \n",
    "\n",
    "# RETURN TO THIS LATER\n",
    "# moviesDF.groupby('genres').size() # prints how many of each genre there exists\n",
    "\n",
    "\n",
    "y = moviesDF[['genres']] # our dependent variable\n",
    "\n",
    "X = moviesDF[['overview']] # our independant variable\n",
    "\n",
    "X_list = X.values.tolist()\n",
    "y_list = y.values.tolist()\n",
    "\n",
    "#result, failed = TransformGenre(y_list)\n",
    "\n",
    "y_ = [] # temp, array for genres that arent empty\n",
    "failedIndexes = [] # array to keep track of the index we had an empty genre, to be used later in deletion.\n",
    "\n",
    "counter = 0\n",
    "# we iterate over y_list to find all genres that arent empty, if one is empty its gonna trigger an exception\n",
    "# which triggers our except clause. The except clause saved the index the error empty genre was located and progresses the counter\n",
    "while counter < len(y_list):\n",
    "    try:\n",
    "        genreObject = y_list[counter]\n",
    "        genreStr = genreObject[0]\n",
    "        strstr = genreStr.split()\n",
    "        indexedstr = strstr[3]\n",
    "        test = re.findall(r'\\w+', indexedstr)    \n",
    "        y_.append(test[0])\n",
    "        counter += 1 \n",
    "    except:\n",
    "        failedIndexes.append(counter)\n",
    "        counter += 1\n",
    "        \n",
    "failedIndexes.sort()\n",
    "failedIndexes = failedIndexes[::-1] # reversing the list\n",
    "# as it turns out when you delete an index from a python list, it collapses the list, so we had to delete the highest index first to circumvent this.\n",
    "for index in failedIndexes:\n",
    "    del X_list[index]\n",
    "    \n",
    "y_list = y_ \n",
    "\n",
    "X_ =  [] # temp list to contain all strings\n",
    "failedIndexes = []\n",
    "counter = 0\n",
    "# Currently X_list contains a collection of collections, this kinda og list-ception is incompatible with the AI algorithm\n",
    "# So we create this small loop to extract the string.\n",
    "while counter < len(X_list):\n",
    "    listLine = X_list[counter]\n",
    "    listString = listLine[0]\n",
    "    if not listString:\n",
    "        failedIndexes.append(counter)\n",
    "        counter += 1\n",
    "        continue\n",
    "    X_.append(listString)\n",
    "    counter += 1\n",
    "    \n",
    "failedIndexes = failedIndexes[::-1]\n",
    "for i in failedIndexes:\n",
    "    del X_[i]\n",
    "    \n",
    "X_list = X_\n",
    "\n",
    "# We had nan values in our X_list, we decided to convert the X_list back to a dataframe\n",
    "# in order to run isnull(), which returns a list of false/true wether an entry is nan or not\n",
    "# with this we simply saved the index of which the nan occured and deleted\n",
    "# it from both our dependant and independant variable.\n",
    "\n",
    "tempDf = pd.DataFrame(X_list)\n",
    "boolList = tempDf.isnull().values\n",
    "\n",
    "badIndexes = []\n",
    "counter = 0\n",
    "while counter < len(boolList):\n",
    "    if boolList[counter][0]:\n",
    "        badIndexes.append(counter)               \n",
    "    counter += 1\n",
    "\n",
    "badIndexes = badIndexes[::-1]\n",
    "\n",
    "for i in badIndexes:\n",
    "    del X_list[i]\n",
    "    del y_list[i]\n",
    "    \n",
    "        \n",
    "\n",
    "# we are splitting our dataset up here for training and later validation.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_list,y_list) \n",
    "\n",
    "# Encoder to encode our dependant variable\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Feature Engineering</h2>\n",
    "<p>Count Vector is a matrix notation of the dataset in \n",
    "which every row represents a document from the corpus,\n",
    "every column represents a term from the corpus, \n",
    "and every cell represents the frequency count of a particular term in a particular document.\n",
    "\n",
    "Here we create the count vectorized object\n",
    "analyzer=word means that we chose to create an n-gram over words compared to chars or char_wb which is a special n-gram\n",
    "when using analyzer=word we can choose a token pattern which is decided in a regular expression \n",
    "in this case the '\\w{1,}' means that it will match a word with at least 1 character length.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', stop_words=stop_words, max_features=5000)\n",
    "\n",
    "count_vect.fit(X_train)\n",
    "\n",
    "\n",
    "\n",
    "# Now we are gonna transform the training and test data with our vectorized object\n",
    "\n",
    "X_train_count = count_vect.transform(X_train)\n",
    "X_test_count = count_vect.transform(X_test)\n",
    "\n",
    "# Now we are gonna use Term Frequency - Inverted Document Frequence (TF-IDF) vectors as features\n",
    "# The score generate by the TF-IDF represents the relatuve importance of a term in a document and the entire corpus.\n",
    "# We generate this score in two steps:\n",
    "# The first computes the normalizeds term frequency (Tf) --- TF(x) = Number of times x appears in the document / total bynber if terms in the document. \n",
    "# the second computes the inverse document frequency  (IDF) --- IDF(x) = log_e(total number of documents / number of documents with term x in it)\n",
    "# as mentioned earlier we could have chosen to use an n-gram composed of words, which we have implemented in line 61.\n",
    "# now we are creating the TF-IDF score based on that n-gram.\n",
    "\n",
    "#tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern='\\w{1,}', max_features=5000)\n",
    "tfidf_vect = TfidfVectorizer(encoding='utf-8',lowercase=True, stop_words=stop_words, sublinear_tf=True, use_idf=True,max_features=5000)\n",
    "tfidf_vect.fit(X_train)\n",
    "X_train_tfidf = tfidf_vect.transform(X_train)\n",
    "X_test_tfidf = tfidf_vect.transform(X_test)\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3),stop_words=stop_words,max_features=5000,lowercase=True)\n",
    "tfidf_vect_ngram.fit(X_train)\n",
    "X_train_tfidf_ngram =  tfidf_vect_ngram.transform(X_train)\n",
    "X_test_tfidf_ngram =  tfidf_vect_ngram.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training and testing</h2>\n",
    "<p></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, Count vectors: 0.44761106454316846\n",
      "NB, tf-idf vectors: 0.4082145850796312\n",
      "SVM, Count vectors: 0.38390611902766136\n",
      "SVM, tf-idf vectors: 0.4082145850796312\n",
      "RF, Count vectors: 0.3788767812238055\n",
      "RF, tf-idf vectors: 0.3897736797988265\n"
     ]
    }
   ],
   "source": [
    "label = convert_column_array_to_normal_array(y_train)\n",
    "accuracy = train_model(classifier = naive_bayes.MultinomialNB(),\n",
    "                       feature_vector_train = X_train_count,\n",
    "                       label = label,\n",
    "                       feature_vector_valid = X_test_count)\n",
    "print(f'NB, Count vectors: {accuracy}')\n",
    "\n",
    "accuracy = train_model(classifier = naive_bayes.MultinomialNB(),\n",
    "                       feature_vector_train = X_train_tfidf,\n",
    "                       label = label,\n",
    "                       feature_vector_valid = X_test_tfidf)\n",
    "print(f'NB, tf-idf vectors: {accuracy}')\n",
    "\n",
    "\n",
    "accuracy = train_model(classifier = svm.SVC(),\n",
    "                       feature_vector_train = X_train_count,\n",
    "                       label = label,\n",
    "                       feature_vector_valid = X_test_count)\n",
    "print(f'SVM, Count vectors: {accuracy}')\n",
    "\n",
    "accuracy = train_model(classifier = svm.SVC(),\n",
    "                       feature_vector_train = X_train_tfidf,\n",
    "                       label = label,\n",
    "                       feature_vector_valid = X_test_tfidf)\n",
    "print(f'SVM, tf-idf vectors: {accuracy}')\n",
    "\n",
    "accuracy = train_model(classifier = ensemble.RandomForestClassifier(),\n",
    "                       feature_vector_train = X_train_count,\n",
    "                       label = label,\n",
    "                       feature_vector_valid = X_test_count)\n",
    "print(f'RF, Count vectors: {accuracy}')\n",
    "\n",
    "accuracy = train_model(classifier = ensemble.RandomForestClassifier(),\n",
    "                       feature_vector_train = X_train_tfidf,\n",
    "                       label = label,\n",
    "                       feature_vector_valid = X_test_tfidf)\n",
    "print(f'RF, tf-idf vectors: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Group</h2>\n",
    "<b>- Mikkel Wexøe Ertbjerg // cph-me209@cphbusiness.dk</b>\n",
    "\n",
    "<b>- Nikolai Sjhøholm Christiansen // cph-nc103@cphbusiness.dk</b>\n",
    "\n",
    "<b>- Nikolaj Dyring Jensen // cph-nj183@cphbusiness.dk</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
